[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/text2vec/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624) 

<div align="center">
  <a href="https://github.com/shibing624/text2vec">
    <img src="https://github.com/shibing624/text2vec/blob/master/docs/t2v-logo.png" height="150" alt="Logo">
  </a>
</div>

-----------------

# Text2vec: Text to Vector
[![PyPI version](https://badge.fury.io/py/text2vec.svg)](https://badge.fury.io/py/text2vec)
[![Downloads](https://pepy.tech/badge/text2vec)](https://pepy.tech/project/text2vec)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.5%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)


**Text2vec**: Text to Vector, Get Sentence Embeddings. æ–‡æœ¬å‘é‡åŒ–ï¼ŒæŠŠæ–‡æœ¬(åŒ…æ‹¬è¯ã€å¥å­ã€æ®µè½)è¡¨å¾ä¸ºå‘é‡çŸ©é˜µã€‚

**text2vec**å®ç°äº†Word2Vecã€RankBM25ã€BERTã€Sentence-BERTã€CoSENTç­‰å¤šç§æ–‡æœ¬è¡¨å¾ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå¹¶åœ¨æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚

### News
[2023/06/15] v1.2.0ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`æ¨¡å‹ï¼Œä½¿ç”¨äº†ä¸­æ–‡NLIæ•°æ®é›†[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)å…¨éƒ¨è¯­æ–™è®­ç»ƒçš„CoSENTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°æå‡æ˜æ˜¾ï¼Œè¯¦è§[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)

[2022/03/12] v1.1.4ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼ŒåŸºäºä¸­æ–‡STSè®­ç»ƒé›†è®­ç»ƒçš„CoSENTåŒ¹é…æ¨¡å‹ã€‚è¯¦è§[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)


**Guide**
- [Feature](#Feature)
- [Evaluation](#Evaluation)
- [Install](#install)
- [Usage](#usage)
- [Contact](#Contact)
- [Reference](#reference)


# Feature
### æ–‡æœ¬å‘é‡è¡¨ç¤ºæ¨¡å‹
- [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ï¼šé€šè¿‡è…¾è®¯AI Labå¼€æºçš„å¤§è§„æ¨¡é«˜è´¨é‡ä¸­æ–‡[è¯å‘é‡æ•°æ®ï¼ˆ800ä¸‡ä¸­æ–‡è¯è½»é‡ç‰ˆï¼‰](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (æ–‡ä»¶åï¼šlight_Tencent_AILab_ChineseEmbedding.bin å¯†ç : taweï¼‰å®ç°è¯å‘é‡æ£€ç´¢ï¼Œæœ¬é¡¹ç›®å®ç°äº†å¥å­ï¼ˆè¯å‘é‡æ±‚å¹³å‡ï¼‰çš„word2vecå‘é‡è¡¨ç¤º
- [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ï¼šæƒè¡¡æ€§èƒ½å’Œæ•ˆç‡çš„å¥å‘é‡è¡¨ç¤ºæ¨¡å‹ï¼Œè®­ç»ƒæ—¶é€šè¿‡æœ‰ç›‘ç£è®­ç»ƒä¸Šå±‚åˆ†ç±»å‡½æ•°ï¼Œæ–‡æœ¬åŒ¹é…é¢„æµ‹æ—¶ç›´æ¥å¥å­å‘é‡åšä½™å¼¦ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå¤ç°äº†Sentence-BERTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
- [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ï¼šCoSENTæ¨¡å‹æå‡ºäº†ä¸€ç§æ’åºçš„æŸå¤±å‡½æ•°ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´è´´è¿‘é¢„æµ‹ï¼Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæ•ˆæœæ¯”Sentence-BERTæ›´å¥½ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†CoSENTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹

è¯¦ç»†æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•è§wiki: [æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
# Evaluation

### æ–‡æœ¬åŒ¹é…

- è‹±æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š

| Arch | Backbone | Model  | English-STS-B | 
| :-- | :--- | :--- | :-: |
| GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 |
| BERT | bert-base-uncased | BERT-base-cls | 20.29 |
| BERT | bert-base-uncased | BERT-base-first_last_avg | 59.04 |
| BERT | bert-base-uncased | BERT-base-first_last_avg-whiten(NLI) | 63.65 |
| SBERT | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls | 73.65 |
| SBERT | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg | 77.96 |
| SBERT | xlm-roberta-base | paraphrase-multilingual-MiniLM-L12-v2 | 84.42 |
| CoSENT | bert-base-uncased | CoSENT-base-first_last_avg | 69.93 |
| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68 |

- ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š

| Arch | Backbone | Model  | ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | QPS |
| :-- | :--- | :--- | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | **72.93** | **79.17** | **60.86** | **80.51** | **68.77**  | 3008 |
| CoSENT | Langboat/mengzi-bert-base | CoSENT-mengzi-base | **50.52** | 72.27 | 78.69 | 12.89 | 80.15 | 58.90 | 2502 |
| CoSENT | bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 80.14 | 68.19 | 2653 |
| SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 | 3365 |
| SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 | **79.42** | 55.59 | 64.82 | 63.15 | 2948 |
| CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext | **50.81** | **71.45** | **79.31** | **61.56** | **81.13** | **68.85** | - |
| SBERT | hfl/chinese-roberta-wwm-ext | SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | - |

- æœ¬é¡¹ç›®releaseæ¨¡å‹çš„ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœï¼š

| Arch | BaseModel                    | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |    Avg    |  QPS  |
| :-- |:-----------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:---------:|:-----:|
| Word2Vec | word2vec                     | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |   33.86   | 23769 |
| SBERT | xlm-roberta-base             | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |   41.99   | 3138  |
| CoSENT | hfl/chinese-macbert-base     | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | 48.25 | 3008  |
| CoSENT | hfl/chinese-lert-large       | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |   48.08   | 2092  |
| CoSENT | nghuyong/ernie-3.0-base-zh   | [shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)                                               | 51.26 | 68.72 | 79.13 | 34.28 | 80.70 |   **62.81**   | 3066  |


è¯´æ˜ï¼š
- ç»“æœå€¼æ˜¯spearmanç³»æ•°
- ç»“æœå‡åªç”¨è¯¥æ•°æ®é›†çš„trainè®­ç»ƒï¼Œåœ¨testä¸Šè¯„ä¼°å¾—åˆ°çš„è¡¨ç°ï¼Œæ²¡ç”¨å¤–éƒ¨æ•°æ®
- `shibing624/text2vec-base-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`hfl/chinese-macbert-base`åœ¨ä¸­æ–‡STS-Bæ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡STS-Bæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„æ¨¡å‹åº“[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼Œä¸­æ–‡é€šç”¨è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `shibing624/text2vec-base-chinese-nli`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨ä¸­æ–‡NLIæ•°æ®é›†[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)å…¨éƒ¨è¯­æ–™è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°SOTAï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„æ¨¡å‹åº“[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBERTæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBERTè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰
- `w2v-light-tencent-chinese`æ˜¯è…¾è®¯è¯å‘é‡çš„Word2Vecæ¨¡å‹ï¼ŒCPUåŠ è½½ä½¿ç”¨ï¼Œé€‚ç”¨äºä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œç¼ºå°‘æ•°æ®çš„å†·å¯åŠ¨æƒ…å†µ
- å„é¢„è®­ç»ƒæ¨¡å‹å‡å¯ä»¥é€šè¿‡transformersè°ƒç”¨ï¼Œå¦‚MacBERTæ¨¡å‹ï¼š`--model_name hfl/chinese-macbert-base` æˆ–è€…robertaæ¨¡å‹ï¼š`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`
- ä¸­æ–‡åŒ¹é…æ•°æ®é›†ä¸‹è½½[é“¾æ¥è§ä¸‹æ–¹](#æ•°æ®é›†)
- ä¸­æ–‡åŒ¹é…ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œpoolingæœ€ä¼˜æ˜¯`EncoderType.FIRST_LAST_AVG`å’Œ`EncoderType.MEAN`ï¼Œä¸¤è€…é¢„æµ‹æ•ˆæœå·®å¼‚å¾ˆå°
- ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœå¤ç°ï¼Œå¯ä»¥ä¸‹è½½ä¸­æ–‡åŒ¹é…æ•°æ®é›†åˆ°`examples/data`ï¼Œè¿è¡Œ[tests/test_model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/test_model_spearman.py)ä»£ç å¤ç°è¯„æµ‹ç»“æœ
- QPSçš„GPUæµ‹è¯•ç¯å¢ƒæ˜¯Tesla V100ï¼Œæ˜¾å­˜32GB

# Demo

Official Demo: https://www.mulanai.com/product/short_text_sim/

HuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec

![](docs/hf.png)

run example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:
```shell
python examples/gradio_demo.py
```

# Install
```shell
pip install torch # conda install pytorch
pip install -U text2vec
```

or

```shell
pip install torch # conda install pytorch
pip install -r requirements.txt

git clone https://github.com/shibing624/text2vec.git
cd text2vec
pip install --no-deps .
```

# Usage

## æ–‡æœ¬å‘é‡è¡¨å¾

åŸºäº`pretrained model`è®¡ç®—æ–‡æœ¬å‘é‡ï¼š

```zsh
>>> from text2vec import SentenceModel
>>> m = SentenceModel()
>>> m.encode("å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡")
Embedding shape: (768,)
```

example: [examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_demo.py)

```python
import sys

sys.path.append('..')
from text2vec import SentenceModel
from text2vec import Word2Vec


def compute_emb(model):
    # Embed a list of sentences
    sentences = [
        'å¡',
        'é“¶è¡Œå¡',
        'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
        'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',
        'This framework generates embeddings for each input sentence',
        'Sentences are passed as a list of string.',
        'The quick brown fox jumps over the lazy dog.'
    ]
    sentence_embeddings = model.encode(sentences)
    print(type(sentence_embeddings), sentence_embeddings.shape)

    # The result is a list of sentence embeddings as numpy arrays
    for sentence, embedding in zip(sentences, sentence_embeddings):
        print("Sentence:", sentence)
        print("Embedding shape:", embedding.shape)
        print("Embedding head:", embedding[:10])
        print()


if __name__ == "__main__":
    # ä¸­æ–‡å¥å‘é‡æ¨¡å‹(CoSENT)ï¼Œä¸­æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
    t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
    compute_emb(t2v_model)

    # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆSentence-BERTï¼‰ï¼Œè‹±æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
    sbert_model = SentenceModel("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    compute_emb(sbert_model)

    # ä¸­æ–‡è¯å‘é‡æ¨¡å‹(word2vec)ï¼Œä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œå†·å¯åŠ¨é€‚ç”¨
    w2v_model = Word2Vec("w2v-light-tencent-chinese")
    compute_emb(w2v_model)

```

output:
```
<class 'numpy.ndarray'> (7, 768)
Sentence: å¡
Embedding shape: (768,)

Sentence: é“¶è¡Œå¡
Embedding shape: (768,)
 ... 
```

- è¿”å›å€¼`embeddings`æ˜¯`numpy.ndarray`ç±»å‹ï¼Œshapeä¸º`(sentences_size, model_embedding_size)`ï¼Œä¸‰ä¸ªæ¨¡å‹ä»»é€‰ä¸€ç§å³å¯ï¼Œæ¨èç”¨ç¬¬ä¸€ä¸ªã€‚
- `shibing624/text2vec-base-chinese`æ¨¡å‹æ˜¯CoSENTæ–¹æ³•åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„
æ¨¡å‹åº“[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼Œ
æ˜¯`text2vec.SentenceModel`æŒ‡å®šçš„é»˜è®¤æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸Šé¢ç¤ºä¾‹è°ƒç”¨ï¼Œæˆ–è€…å¦‚ä¸‹æ‰€ç¤ºç”¨[transformersåº“](https://github.com/huggingface/transformers)è°ƒç”¨ï¼Œ
æ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.cache/huggingface/transformers`
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯Sentence-BERTçš„å¤šè¯­è¨€å¥å‘é‡æ¨¡å‹ï¼Œ
é€‚ç”¨äºé‡Šä¹‰ï¼ˆparaphraseï¼‰è¯†åˆ«ï¼Œæ–‡æœ¬åŒ¹é…ï¼Œé€šè¿‡`text2vec.SentenceModel`å’Œ[sentence-transformersåº“]((https://github.com/UKPLab/sentence-transformers))éƒ½å¯ä»¥è°ƒç”¨è¯¥æ¨¡å‹
- `w2v-light-tencent-chinese`æ˜¯é€šè¿‡gensimåŠ è½½çš„Word2Vecæ¨¡å‹ï¼Œä½¿ç”¨è…¾è®¯è¯å‘é‡`Tencent_AILab_ChineseEmbedding.tar.gz`è®¡ç®—å„å­—è¯çš„è¯å‘é‡ï¼Œå¥å­å‘é‡é€šè¿‡å•è¯è¯
å‘é‡å–å¹³å‡å€¼å¾—åˆ°ï¼Œæ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`

#### Usage (HuggingFace Transformers)
Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 

First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.

example: [examples/use_origin_transformers_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/use_origin_transformers_demo.py)

```python
import os
import torch
from transformers import AutoTokenizer, AutoModel

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


# Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained('shibing624/text2vec-base-chinese')
model = AutoModel.from_pretrained('shibing624/text2vec-base-chinese')
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']
# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)
# Perform pooling. In this case, max pooling.
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
print("Sentence embeddings:")
print(sentence_embeddings)
```

#### Usage (sentence-transformers)
[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.

Install sentence-transformers:
```shell
pip install -U sentence-transformers
```
Then load model and predict:
```python
from sentence_transformers import SentenceTransformer

m = SentenceTransformer("shibing624/text2vec-base-chinese")
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']

sentence_embeddings = m.encode(sentences)
print("Sentence embeddings:")
print(sentence_embeddings)
```

#### `Word2Vec`è¯å‘é‡

æä¾›ä¸¤ç§`Word2Vec`è¯å‘é‡ï¼Œä»»é€‰ä¸€ä¸ªï¼š

  - è½»é‡ç‰ˆè…¾è®¯è¯å‘é‡ [ç™¾åº¦äº‘ç›˜-å¯†ç :tawe](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) æˆ– [è°·æ­Œäº‘ç›˜](https://drive.google.com/u/0/uc?id=1iQo9tBb2NgFOBxx0fA16AZpSgc-bG_Rp&export=download)ï¼ŒäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œ111Mï¼Œæ˜¯ç®€åŒ–åçš„é«˜é¢‘143613ä¸ªè¯ï¼Œæ¯ä¸ªè¯å‘é‡è¿˜æ˜¯200ç»´ï¼ˆè·ŸåŸç‰ˆä¸€æ ·ï¼‰ï¼Œè¿è¡Œç¨‹åºï¼Œè‡ªåŠ¨ä¸‹è½½åˆ° `~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`
  - è…¾è®¯è¯å‘é‡-å®˜æ–¹å…¨é‡, 6.78Gæ”¾åˆ°ï¼š `~/.text2vec/datasets/Tencent_AILab_ChineseEmbedding.txt`ï¼Œè…¾è®¯è¯å‘é‡ä¸»é¡µï¼šhttps://ai.tencent.com/ailab/nlp/zh/index.html è¯å‘é‡ä¸‹è½½åœ°å€ï¼šhttps://ai.tencent.com/ailab/nlp/en/download.html  æ›´å¤šæŸ¥çœ‹[è…¾è®¯è¯å‘é‡ä»‹ç»-wiki](https://github.com/shibing624/text2vec/wiki/%E8%85%BE%E8%AE%AF%E8%AF%8D%E5%90%91%E9%87%8F%E4%BB%8B%E7%BB%8D)



## ä¸‹æ¸¸ä»»åŠ¡
### 1. å¥å­ç›¸ä¼¼åº¦è®¡ç®—

example: [examples/semantic_text_similarity_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_text_similarity_demo.py)

```python
import sys

sys.path.append('..')
from text2vec import Similarity

# Two lists of sentences
sentences1 = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
              'The cat sits outside',
              'A man is playing guitar',
              'The new movie is awesome']

sentences2 = ['èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',
              'The dog plays in the garden',
              'A woman watches TV',
              'The new movie is so great']

sim_model = Similarity()
for i in range(len(sentences1)):
    for j in range(len(sentences2)):
        score = sim_model.get_score(sentences1[i], sentences2[j])
        print("{} \t\t {} \t\t Score: {:.4f}".format(sentences1[i], sentences2[j], score))
```

output:
```shell
å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ 		 èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ 		 Score: 0.9477
å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ 		 The dog plays in the garden 		 Score: -0.1748
å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ 		 A woman watches TV 		 Score: -0.0839
å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ 		 The new movie is so great 		 Score: -0.0044
The cat sits outside 		 èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ 		 Score: -0.0097
The cat sits outside 		 The dog plays in the garden 		 Score: 0.1908
The cat sits outside 		 A woman watches TV 		 Score: -0.0203
The cat sits outside 		 The new movie is so great 		 Score: 0.0302
A man is playing guitar 		 èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ 		 Score: -0.0010
A man is playing guitar 		 The dog plays in the garden 		 Score: 0.1062
A man is playing guitar 		 A woman watches TV 		 Score: 0.0055
A man is playing guitar 		 The new movie is so great 		 Score: 0.0097
The new movie is awesome 		 èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ 		 Score: 0.0302
The new movie is awesome 		 The dog plays in the garden 		 Score: -0.0160
The new movie is awesome 		 A woman watches TV 		 Score: 0.1321
The new movie is awesome 		 The new movie is so great 		 Score: 0.9591
```

> å¥å­ä½™å¼¦ç›¸ä¼¼åº¦å€¼`score`èŒƒå›´æ˜¯[-1, 1]ï¼Œå€¼è¶Šå¤§è¶Šç›¸ä¼¼ã€‚

### 2. æ–‡æœ¬åŒ¹é…æœç´¢

ä¸€èˆ¬åœ¨æ–‡æ¡£å€™é€‰é›†ä¸­æ‰¾ä¸queryæœ€ç›¸ä¼¼çš„æ–‡æœ¬ï¼Œå¸¸ç”¨äºQAåœºæ™¯çš„é—®å¥ç›¸ä¼¼åŒ¹é…ã€æ–‡æœ¬ç›¸ä¼¼æ£€ç´¢ç­‰ä»»åŠ¡ã€‚


example: [examples/semantic_search_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_search_demo.py)

```python
import sys

sys.path.append('..')
from text2vec import SentenceModel, cos_sim, semantic_search

embedder = SentenceModel()

# Corpus with example sentences
corpus = [
    'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',
    'æˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘—',
    'A man is eating food.',
    'A man is eating a piece of bread.',
    'The girl is carrying a baby.',
    'A man is riding a horse.',
    'A woman is playing violin.',
    'Two men pushed carts through the woods.',
    'A man is riding a white horse on an enclosed ground.',
    'A monkey is playing drums.',
    'A cheetah is running behind its prey.'
]
corpus_embeddings = embedder.encode(corpus)

# Query sentences:
queries = [
    'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
    'A man is eating pasta.',
    'Someone in a gorilla costume is playing a set of drums.',
    'A cheetah chases prey on across a field.']

for query in queries:
    query_embedding = embedder.encode(query)
    hits = semantic_search(query_embedding, corpus_embeddings, top_k=5)
    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")
    hits = hits[0]  # Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
```
output:
```shell
Query: å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡
Top 5 most similar sentences in corpus:
èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ (Score: 0.9477)
æˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘— (Score: 0.3635)
A man is eating food. (Score: 0.0321)
A man is riding a horse. (Score: 0.0228)
Two men pushed carts through the woods. (Score: 0.0090)

======================
Query: A man is eating pasta.
Top 5 most similar sentences in corpus:
A man is eating food. (Score: 0.6734)
A man is eating a piece of bread. (Score: 0.4269)
A man is riding a horse. (Score: 0.2086)
A man is riding a white horse on an enclosed ground. (Score: 0.1020)
A cheetah is running behind its prey. (Score: 0.0566)

======================
Query: Someone in a gorilla costume is playing a set of drums.
Top 5 most similar sentences in corpus:
A monkey is playing drums. (Score: 0.8167)
A cheetah is running behind its prey. (Score: 0.2720)
A woman is playing violin. (Score: 0.1721)
A man is riding a horse. (Score: 0.1291)
A man is riding a white horse on an enclosed ground. (Score: 0.1213)

======================
Query: A cheetah chases prey on across a field.
Top 5 most similar sentences in corpus:
A cheetah is running behind its prey. (Score: 0.9147)
A monkey is playing drums. (Score: 0.2655)
A man is riding a horse. (Score: 0.1933)
A man is riding a white horse on an enclosed ground. (Score: 0.1733)
A man is eating food. (Score: 0.0329)
```


## ä¸‹æ¸¸ä»»åŠ¡æ”¯æŒåº“
**similaritiesåº“[æ¨è]**

æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å’Œæ–‡æœ¬åŒ¹é…æœç´¢ä»»åŠ¡ï¼Œæ¨èä½¿ç”¨ [similaritiesåº“](https://github.com/shibing624/similarities) ï¼Œå…¼å®¹æœ¬é¡¹ç›®releaseçš„
Word2vecã€SBERTã€Cosentç±»è¯­ä¹‰åŒ¹é…æ¨¡å‹ï¼Œè¿˜æ”¯æŒå­—é¢ç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—ã€åŒ¹é…æœç´¢ç®—æ³•ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒã€‚

å®‰è£…ï¼š
```pip install -U similarities```

å¥å­ç›¸ä¼¼åº¦è®¡ç®—ï¼š
```python
from similarities import Similarity

m = Similarity()
r = m.similarity('å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')
print(f"similarity score: {float(r)}")  # similarity score: 0.855146050453186
```

# Models

## CoSENT model

CoSENTï¼ˆCosine Sentenceï¼‰æ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨Sentence-BERTä¸Šæ”¹è¿›äº†CosineRankLossçš„å¥å‘é‡æ–¹æ¡ˆ


Network structure:

Training:

<img src="docs/cosent_train.png" width="300" />


Inference:

<img src="docs/inference.png" width="300" />

#### CoSENT ç›‘ç£æ¨¡å‹
è®­ç»ƒå’Œé¢„æµ‹CoSENTæ¨¡å‹ï¼š

- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹

example: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)

```shell
cd examples
python training_sup_text_matching_model.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-cosent
```

- åœ¨èš‚èšé‡‘èåŒ¹é…æ•°æ®é›†ATECä¸Šè®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹

æ”¯æŒè¿™äº›ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„ä½¿ç”¨ï¼š'ATEC', 'STS-B', 'BQ', 'LCQMC', 'PAWSX'ï¼Œå…·ä½“å‚è€ƒHuggingFace datasets [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)
```shell
python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent
```

- åœ¨è‡ªæœ‰ä¸­æ–‡æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹

example: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)

```shell
python training_sup_text_matching_model_mydata.py --do_train --do_predict
```

è®­ç»ƒé›†æ ¼å¼å‚è€ƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)

```shell
sentence1   sentence2   label
ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚	ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚	2
ä¸€ç¾¤ç”·äººåœ¨æµ·æ»©ä¸Šè¸¢è¶³çƒã€‚	ä¸€ç¾¤ç”·å­©åœ¨æµ·æ»©ä¸Šè¸¢è¶³çƒã€‚	3
ä¸€ä¸ªå¥³äººåœ¨æµ‹é‡å¦ä¸€ä¸ªå¥³äººçš„è„šè¸ã€‚	å¥³äººæµ‹é‡å¦ä¸€ä¸ªå¥³äººçš„è„šè¸ã€‚	5
```

`label`å¯ä»¥æ˜¯0ï¼Œ1æ ‡ç­¾ï¼Œ0ä»£è¡¨ä¸¤ä¸ªå¥å­ä¸ç›¸ä¼¼ï¼Œ1ä»£è¡¨ç›¸ä¼¼ï¼›ä¹Ÿå¯ä»¥æ˜¯0-5çš„è¯„åˆ†ï¼Œè¯„åˆ†è¶Šé«˜ï¼Œè¡¨ç¤ºä¸¤ä¸ªå¥å­è¶Šç›¸ä¼¼ã€‚æ¨¡å‹éƒ½èƒ½æ”¯æŒã€‚


- åœ¨è‹±æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹

example: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)

```shell
cd examples
python training_sup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased  --output_dir ./outputs/STS-B-en-cosent
```

#### CoSENT æ— ç›‘ç£æ¨¡å‹
- åœ¨è‹±æ–‡NLIæ•°æ®é›†è®­ç»ƒ`CoSENT`æ¨¡å‹ï¼Œåœ¨STS-Bæµ‹è¯•é›†è¯„ä¼°æ•ˆæœ

example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)

```shell
cd examples
python training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent
```


## Sentence-BERT model

Sentence-BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œè¡¨å¾å¼å¥å‘é‡è¡¨ç¤ºæ–¹æ¡ˆ

Network structure:

Training:

<img src="docs/sbert_train.png" width="300" />


Inference:

<img src="docs/sbert_inference.png" width="300" />

#### SentenceBERT ç›‘ç£æ¨¡å‹
- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`SBERT`æ¨¡å‹

example: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)

```shell
cd examples
python training_sup_text_matching_model.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-sbert
```
- åœ¨è‹±æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`SBERT`æ¨¡å‹

example: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)

```shell
cd examples
python training_sup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-sbert
```

#### SentenceBERT æ— ç›‘ç£æ¨¡å‹
- åœ¨è‹±æ–‡NLIæ•°æ®é›†è®­ç»ƒ`SBERT`æ¨¡å‹ï¼Œåœ¨STS-Bæµ‹è¯•é›†è¯„ä¼°æ•ˆæœ

example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)

```shell
cd examples
python training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert
```

## BERT-Match model
BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼ŒåŸç”ŸBERTåŒ¹é…ç½‘ç»œç»“æ„ï¼Œäº¤äº’å¼å¥å‘é‡åŒ¹é…æ¨¡å‹

Network structure:

Training and inference:

<img src="docs/bert-fc-train.png" width="300" />

è®­ç»ƒè„šæœ¬åŒä¸Š[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ã€‚


## æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰

ç”±äºtext2vecè®­ç»ƒçš„æ¨¡å‹å¯ä»¥ä½¿ç”¨[sentence-transformers](https://github.com/UKPLab/sentence-transformers)åº“åŠ è½½ï¼Œæ­¤å¤„å¤ç”¨å…¶æ¨¡å‹è’¸é¦æ–¹æ³•[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)ã€‚

1. æ¨¡å‹é™ç»´ï¼Œå‚è€ƒ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)ä½¿ç”¨PCAå¯¹æ¨¡å‹è¾“å‡ºembeddingé™ç»´ï¼Œå¯å‡å°‘milvusç­‰å‘é‡æ£€ç´¢æ•°æ®åº“çš„å­˜å‚¨å‹åŠ›ï¼Œè¿˜èƒ½è½»å¾®æå‡æ¨¡å‹æ•ˆæœã€‚
2. æ¨¡å‹è’¸é¦ï¼Œå‚è€ƒ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)ä½¿ç”¨è’¸é¦æ–¹æ³•ï¼Œå°†Teacherå¤§æ¨¡å‹è’¸é¦åˆ°æ›´å°‘layerså±‚æ•°çš„studentæ¨¡å‹ä¸­ï¼Œåœ¨æƒè¡¡æ•ˆæœçš„æƒ…å†µä¸‹ï¼Œå¯å¤§å¹…æå‡æ¨¡å‹é¢„æµ‹é€Ÿåº¦ã€‚

## æ¨¡å‹éƒ¨ç½²

æä¾›ä¸¤ç§éƒ¨ç½²æ¨¡å‹ï¼Œæ­å»ºæœåŠ¡çš„æ–¹æ³•ï¼š 1ï¼‰åŸºäºJinaæ­å»ºgRPCæœåŠ¡ã€æ¨èã€‘ï¼›2ï¼‰åŸºäºFastAPIæ­å»ºåŸç”ŸHttpæœåŠ¡ã€‚

### JinaæœåŠ¡
é‡‡ç”¨C/Sæ¨¡å¼æ­å»ºé«˜æ€§èƒ½æœåŠ¡ï¼Œæ”¯æŒdockeräº‘åŸç”Ÿï¼ŒgRPC/HTTP/WebSocketï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹åŒæ—¶é¢„æµ‹ï¼ŒGPUå¤šå¡å¤„ç†ã€‚

- å®‰è£…ï¼š
```pip install jina```

- å¯åŠ¨æœåŠ¡ï¼š

example: [examples/jina_server_demo.py](examples/jina_server_demo.py)
```python
from jina import Flow

port = 50001
f = Flow(port=port).add(
    uses='jinahub://Text2vecEncoder',
    uses_with={'model_name': 'shibing624/text2vec-base-chinese'}
)

with f:
    # backend server forever
    f.block()
```

è¯¥æ¨¡å‹é¢„æµ‹æ–¹æ³•ï¼ˆexecutorï¼‰å·²ç»ä¸Šä¼ åˆ°[JinaHub](https://hub.jina.ai/executor/eq45c9uq)ï¼Œé‡Œé¢åŒ…æ‹¬dockerã€k8séƒ¨ç½²æ–¹æ³•ã€‚

- è°ƒç”¨æœåŠ¡ï¼š


```python
from jina import Client
from docarray import Document, DocumentArray

port = 50001

c = Client(port=port)

data = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
        'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']
print("data:", data)
print('data embs:')
r = c.post('/', inputs=DocumentArray([Document(text='å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡'), Document(text='èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')]))
print(r.embeddings)
```

æ‰¹é‡è°ƒç”¨æ–¹æ³•è§example: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)


### FastAPIæœåŠ¡

- å®‰è£…ï¼š
```pip install fastapi uvicorn```

- å¯åŠ¨æœåŠ¡ï¼š

example: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)
```shell
cd examples
python fastapi_server_demo.py
```

- è°ƒç”¨æœåŠ¡ï¼š
```shell
curl -X 'GET' \
  'http://0.0.0.0:8001/emb?q=hello' \
  -H 'accept: application/json'
```


## æ•°æ®é›†

- æœ¬é¡¹ç›®releaseçš„æ•°æ®é›†ï¼š

| Dataset               | Introduce                                                           | Download Link                                                                                                                                                                                                                                                                                         |
|:----------------------|:--------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| shibing624/nli-zh-all | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®åˆé›†ï¼Œæ•´åˆäº†æ–‡æœ¬æ¨ç†ï¼Œç›¸ä¼¼ï¼Œæ‘˜è¦ï¼Œé—®ç­”ï¼ŒæŒ‡ä»¤å¾®è°ƒç­‰ä»»åŠ¡çš„820ä¸‡é«˜è´¨é‡æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºåŒ¹é…æ ¼å¼æ•°æ®é›†           | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |
| shibing624/snli-zh    | ä¸­æ–‡SNLIå’ŒMultiNLIæ•°æ®é›†ï¼Œç¿»è¯‘è‡ªè‹±æ–‡SNLIå’ŒMultiNLI                                       | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |
| shibing624/nli_zh     | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®é›†ï¼Œæ•´åˆäº†ä¸­æ–‡ATECã€BQã€LCQMCã€PAWSXã€STS-Bå…±5ä¸ªä»»åŠ¡çš„æ•°æ®é›†                   | [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) </br> or </br> [ç™¾åº¦ç½‘ç›˜(æå–ç :qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ) </br> or </br> [github](https://github.com/shibing624/text2vec/releases/download/1.1.2/senteval_cn.zip) </br> |
| ATEC                  | ä¸­æ–‡ATECæ•°æ®é›†ï¼Œèš‚èšé‡‘æœQ-Qpairæ•°æ®é›†                                            | [ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC)                                                                                                                                                                                                                                 |
| BQ                    | ä¸­æ–‡BQ(Bank Question)æ•°æ®é›†ï¼Œé“¶è¡ŒQ-Qpairæ•°æ®é›†                                 | [BQ](http://icrc.hitsz.edu.cn/info/1037/1162.htm)                                                                                                                                                                                                                                                     |
| LCQMC                 | ä¸­æ–‡LCQMC(large-scale Chinese question matching corpus)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›† | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |
| PAWSX                 | ä¸­æ–‡PAWS(Paraphrase Adversaries from Word Scrambling)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†   | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |
| STS-B                 | ä¸­æ–‡STS-Bæ•°æ®é›†ï¼Œä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ï¼Œä»è‹±æ–‡STS-Bç¿»è¯‘ä¸ºä¸­æ–‡çš„æ•°æ®é›†                            | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |


å¸¸ç”¨è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼š

- å¤§åé¼é¼çš„multi_nliå’Œsnli: https://huggingface.co/datasets/multi_nli
- å¤§åé¼é¼çš„multi_nliå’Œsnli: https://huggingface.co/datasets/snli
- https://huggingface.co/datasets/metaeval/cnli
- https://huggingface.co/datasets/mteb/stsbenchmark-sts
- https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli
- https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7


æ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹ï¼š
```shell
pip install datasets
```

```python
from datasets import load_dataset

dataset = load_dataset("shibing624/nli_zh", "STS-B") # ATEC or BQ or LCQMC or PAWSX or STS-B
print(dataset)
print(dataset['test'][0])
```

output:
```shell
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label'],
        num_rows: 5231
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label'],
        num_rows: 1458
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label'],
        num_rows: 1361
    })
})
{'sentence1': 'ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚', 'sentence2': 'ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚', 'label': 2}
```





# Contact

- Issue(å»ºè®®)ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
- å¾®ä¿¡æˆ‘ï¼šåŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸-NLP* è¿›NLPäº¤æµç¾¤ã€‚

<img src="docs/wechat.jpeg" width="200" />


# Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†text2vecï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

APA:
```latex
Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec
```

BibTeX:
```latex
@misc{Text2vec,
  author = {Xu, Ming},
  title = {Text2vec: Text to vector toolkit},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/shibing624/text2vec}},
}
```

# License


æˆæƒåè®®ä¸º [The Apache License 2.0](LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ text2vecçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚


# Contribute
é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š

 - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
 - ä½¿ç”¨`python -m pytest -v`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„

ä¹‹åå³å¯æäº¤PRã€‚

# Reference
- [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸Šï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10335164.html)
- [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸‹ï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10341841.html)
- [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)
- [å››ç§è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ–¹æ³•å¯¹æ¯”[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)
- [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)
- [CoSENTï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ](https://kexue.fm/archives/8847)
- [è°ˆè°ˆæ–‡æœ¬åŒ¹é…å’Œå¤šè½®æ£€ç´¢](https://zhuanlan.zhihu.com/p/111769969)
- [Sentence-transformers](https://www.sbert.net/examples/applications/computing-embeddings/README.html)
