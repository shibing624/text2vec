[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](https://github.com/shibing624/text2vec/blob/master/README.md) | [**ğŸŒEnglish**](https://github.com/shibing624/text2vec/blob/master/README_EN.md) | [**ğŸ“–æ–‡æ¡£/Docs**](https://github.com/shibing624/text2vec/wiki) | [**ğŸ¤–æ¨¡å‹/Models**](https://huggingface.co/shibing624) 

<div align="center">
  <a href="https://github.com/shibing624/text2vec">
    <img src="https://github.com/shibing624/text2vec/blob/master/docs/t2v-logo.png" height="150" alt="Logo">
  </a>
</div>

-----------------

# Text2vec: Text to Vector
[![PyPI version](https://badge.fury.io/py/text2vec.svg)](https://badge.fury.io/py/text2vec)
[![Downloads](https://static.pepy.tech/badge/text2vec)](https://pepy.tech/project/text2vec)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![python_version](https://img.shields.io/badge/Python-3.5%2B-green.svg)](requirements.txt)
[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
[![Wechat Group](https://img.shields.io/badge/wechat-group-green.svg?logo=wechat)](#Contact)


**Text2vec**: Text to Vector, Get Sentence Embeddings. æ–‡æœ¬å‘é‡åŒ–ï¼ŒæŠŠæ–‡æœ¬(åŒ…æ‹¬è¯ã€å¥å­ã€æ®µè½)è¡¨å¾ä¸ºå‘é‡çŸ©é˜µã€‚

**text2vec**å®ç°äº†Word2Vecã€RankBM25ã€BERTã€Sentence-BERTã€CoSENTç­‰å¤šç§æ–‡æœ¬è¡¨å¾ã€æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å‹ï¼Œå¹¶åœ¨æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼ˆç›¸ä¼¼åº¦è®¡ç®—ï¼‰ä»»åŠ¡ä¸Šæ¯”è¾ƒäº†å„æ¨¡å‹çš„æ•ˆæœã€‚

### News
[2023/09/20] v1.2.9ç‰ˆæœ¬: æ”¯æŒå¤šå¡æ¨ç†ï¼ˆå¤šè¿›ç¨‹å®ç°å¤šGPUã€å¤šCPUæ¨ç†ï¼‰ï¼Œæ–°å¢å‘½ä»¤è¡Œå·¥å…·ï¼ˆCLIï¼‰ï¼Œå¯ä»¥è„šæœ¬æ‰§è¡Œæ‰¹é‡æ–‡æœ¬å‘é‡åŒ–ï¼Œè¯¦è§[Release-v1.2.9](https://github.com/shibing624/text2vec/releases/tag/1.2.9)

[2023/09/03] v1.2.4ç‰ˆæœ¬: æ”¯æŒFlagEmbeddingæ¨¡å‹è®­ç»ƒï¼Œå‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-bge-large-chinese](https://huggingface.co/shibing624/text2vec-bge-large-chinese)ï¼Œç”¨CoSENTæ–¹æ³•ç›‘ç£è®­ç»ƒï¼ŒåŸºäº`BAAI/bge-large-zh-noinstruct`ç”¨ä¸­æ–‡åŒ¹é…æ•°æ®é›†è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼ŒçŸ­æ–‡æœ¬åŒºåˆ†åº¦ä¸Šæå‡æ˜æ˜¾ï¼Œè¯¦è§[Release-v1.2.4](https://github.com/shibing624/text2vec/releases/tag/1.2.4)

[2023/07/17] v1.2.2ç‰ˆæœ¬: æ”¯æŒå¤šå¡è®­ç»ƒï¼Œå‘å¸ƒäº†å¤šè¯­è¨€åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)ï¼Œç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¯¦è§[Release-v1.2.2](https://github.com/shibing624/text2vec/releases/tag/1.2.2)

[2023/06/19] v1.2.1ç‰ˆæœ¬: æ›´æ–°äº†ä¸­æ–‡åŒ¹é…æ¨¡å‹`shibing624/text2vec-base-chinese-nli`ä¸ºæ–°ç‰ˆ[shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)ï¼Œé’ˆå¯¹CoSENTçš„lossè®¡ç®—å¯¹æ’åºæ•æ„Ÿç‰¹ç‚¹ï¼Œäººå·¥æŒ‘é€‰å¹¶æ•´ç†å‡ºé«˜è´¨é‡çš„æœ‰ç›¸å…³æ€§æ’åºçš„STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°ç›¸å¯¹ä¹‹å‰æœ‰æå‡ï¼›å‘å¸ƒäº†é€‚ç”¨äºs2pçš„ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)ï¼Œè¯¦è§[Release-v1.2.1](https://github.com/shibing624/text2vec/releases/tag/1.2.1)

[2023/06/15] v1.2.0ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese-nli](https://huggingface.co/shibing624/text2vec-base-chinese-nli)ï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`æ¨¡å‹ï¼Œä½¿ç”¨äº†ä¸­æ–‡NLIæ•°æ®é›†[shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)å…¨éƒ¨è¯­æ–™è®­ç»ƒçš„CoSENTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨å„è¯„ä¼°é›†è¡¨ç°æå‡æ˜æ˜¾ï¼Œè¯¦è§[Release-v1.2.0](https://github.com/shibing624/text2vec/releases/tag/1.2.0)

[2022/03/12] v1.1.4ç‰ˆæœ¬: å‘å¸ƒäº†ä¸­æ–‡åŒ¹é…æ¨¡å‹[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼ŒåŸºäºä¸­æ–‡STSè®­ç»ƒé›†è®­ç»ƒçš„CoSENTåŒ¹é…æ¨¡å‹ã€‚è¯¦è§[Release-v1.1.4](https://github.com/shibing624/text2vec/releases/tag/1.1.4)


**Guide**
- [Features](#Features)
- [Evaluation](#Evaluation)
- [Install](#install)
- [Usage](#usage)
- [Contact](#Contact)
- [References](#references)


## Features
### æ–‡æœ¬å‘é‡è¡¨ç¤ºæ¨¡å‹
- [Word2Vec](https://github.com/shibing624/text2vec/blob/master/text2vec/word2vec.py)ï¼šé€šè¿‡è…¾è®¯AI Labå¼€æºçš„å¤§è§„æ¨¡é«˜è´¨é‡ä¸­æ–‡[è¯å‘é‡æ•°æ®ï¼ˆ800ä¸‡ä¸­æ–‡è¯è½»é‡ç‰ˆï¼‰](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (æ–‡ä»¶åï¼šlight_Tencent_AILab_ChineseEmbedding.bin å¯†ç : taweï¼‰å®ç°è¯å‘é‡æ£€ç´¢ï¼Œæœ¬é¡¹ç›®å®ç°äº†å¥å­ï¼ˆè¯å‘é‡æ±‚å¹³å‡ï¼‰çš„word2vecå‘é‡è¡¨ç¤º
- [SBERT(Sentence-BERT)](https://github.com/shibing624/text2vec/blob/master/text2vec/sentencebert_model.py)ï¼šæƒè¡¡æ€§èƒ½å’Œæ•ˆç‡çš„å¥å‘é‡è¡¨ç¤ºæ¨¡å‹ï¼Œè®­ç»ƒæ—¶é€šè¿‡æœ‰ç›‘ç£è®­ç»ƒBERTå’Œsoftmaxåˆ†ç±»å‡½æ•°ï¼Œæ–‡æœ¬åŒ¹é…é¢„æµ‹æ—¶ç›´æ¥å–å¥å­å‘é‡åšä½™å¼¦ï¼Œå¥å­è¡¨å¾æ–¹æ³•ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå¤ç°äº†Sentence-BERTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
- [CoSENT(Cosine Sentence)](https://github.com/shibing624/text2vec/blob/master/text2vec/cosent_model.py)ï¼šCoSENTæ¨¡å‹æå‡ºäº†ä¸€ç§æ’åºçš„æŸå¤±å‡½æ•°ï¼Œä½¿è®­ç»ƒè¿‡ç¨‹æ›´è´´è¿‘é¢„æµ‹ï¼Œæ¨¡å‹æ”¶æ•›é€Ÿåº¦å’Œæ•ˆæœæ¯”Sentence-BERTæ›´å¥½ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†CoSENTæ¨¡å‹çš„è®­ç»ƒå’Œé¢„æµ‹
- [BGE(BAAI general embedding)](https://github.com/shibing624/text2vec/blob/master/text2vec/bge_model.py)ï¼šBGEæ¨¡å‹æŒ‰ç…§[retromae](https://github.com/staoxiao/RetroMAE)æ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒï¼Œ[å‚è€ƒè®ºæ–‡](https://aclanthology.org/2022.emnlp-main.35.pdf)ï¼Œå†ä½¿ç”¨å¯¹æ¯”å­¦ä¹ finetuneå¾®è°ƒè®­ç»ƒæ¨¡å‹ï¼Œæœ¬é¡¹ç›®åŸºäºPyTorchå®ç°äº†BGEæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒå’Œé¢„æµ‹


è¯¦ç»†æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•è§wiki: [æ–‡æœ¬å‘é‡è¡¨ç¤ºæ–¹æ³•](https://github.com/shibing624/text2vec/wiki/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95)
## Evaluation

æ–‡æœ¬åŒ¹é…

#### è‹±æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š


| Arch   | BaseModel                                        | Model                                                                                                                | English-STS-B | 
|:-------|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-------------:|
| GloVe  | glove                                           | Avg_word_embeddings_glove_6B_300d                                                                                    |     61.77     |
| BERT   | bert-base-uncased                               | BERT-base-cls                                                                                                        |     20.29     |
| BERT   | bert-base-uncased                               | BERT-base-first_last_avg                                                                                             |     59.04     |
| BERT   | bert-base-uncased                               | BERT-base-first_last_avg-whiten(NLI)                                                                                 |     63.65     |
| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls                                                                                                   |     73.65     |
| SBERT  | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg                                                                                        |     77.96     |
| CoSENT | bert-base-uncased                               | CoSENT-base-first_last_avg                                                                                           |     69.93     |
| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg                                                                                       |     79.68     |
| CoSENT | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                |     80.12     |

#### ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„è¯„æµ‹ç»“æœï¼š


| Arch   | BaseModel                    | Model           | ATEC  |  BQ   | LCQMC | PAWSX | STS-B |  Avg  | 
|:-------|:----------------------------|:--------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
| SBERT  | bert-base-chinese           | SBERT-bert-base     | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 |
| SBERT  | hfl/chinese-macbert-base    | SBERT-macbert-base  | 47.28 | 68.63 | 79.42 | 55.59 | 64.82 | 63.15 |
| SBERT  | hfl/chinese-roberta-wwm-ext | SBERT-roberta-ext   | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 |
| CoSENT | bert-base-chinese           | CoSENT-bert-base    | 49.74 | 72.38 | 78.69 | 60.00 | 79.27 | 68.01 |
| CoSENT | hfl/chinese-macbert-base    | CoSENT-macbert-base | 50.39 | 72.93 | 79.17 | 60.86 | 79.30 | 68.53 |
| CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext  | 50.81 | 71.45 | 79.31 | 61.56 | 79.96 | 68.61 |

è¯´æ˜ï¼š
- ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
- ä¸ºè¯„æµ‹æ¨¡å‹èƒ½åŠ›ï¼Œç»“æœå‡åªç”¨è¯¥æ•°æ®é›†çš„trainè®­ç»ƒï¼Œåœ¨testä¸Šè¯„ä¼°å¾—åˆ°çš„è¡¨ç°ï¼Œæ²¡ç”¨å¤–éƒ¨æ•°æ®
- `SBERT-macbert-base`æ¨¡å‹ï¼Œæ˜¯ç”¨SBertæ–¹æ³•è®­ç»ƒï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBertè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰


### Release Models
- æœ¬é¡¹ç›®releaseæ¨¡å‹çš„ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœï¼š

| Arch       | BaseModel                                                   | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
|:-----------|:------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
| Word2Vec   | word2vec                                                    | [w2v-light-tencent-chinese](https://modelscope.cn/models/lili666/text2vec-word2vec-tencent-chinese)                                               | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
| SBERT      | xlm-roberta-base                                            | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
| CoSENT     | hfl/chinese-macbert-base                                    | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
| CoSENT     | hfl/chinese-lert-large                                      | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
| CoSENT     | nghuyong/ernie-3.0-base-zh                                  | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
| CoSENT     | nghuyong/ernie-3.0-base-zh                                  | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  | **63.08** | 3066  |
| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 3138  |
| CoSENT     | BAAI/bge-large-zh-noinstruct                                | [shibing624/text2vec-bge-large-chinese](https://huggingface.co/shibing624/text2vec-bge-large-chinese)                                             | 38.41 | 61.34 | 71.72 | 35.15 | 76.44 |  71.81  |  63.15  |   59.72   |  844  |


è¯´æ˜ï¼š
- ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
- `shibing624/text2vec-base-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`hfl/chinese-macbert-base`åœ¨ä¸­æ–‡STS-Bæ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡STS-Bæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡é€šç”¨è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `shibing624/text2vec-base-chinese-sentence`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `shibing624/text2vec-base-chinese-paraphrase`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ï¼Œæ•°æ®é›†ç›¸å¯¹äº[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)åŠ å…¥äº†s2p(sentence to paraphrase)æ•°æ®ï¼Œå¼ºåŒ–äº†å…¶é•¿æ–‡æœ¬çš„è¡¨å¾èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°SOTAï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2p(å¥å­vsæ®µè½)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `shibing624/text2vec-base-multilingual`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`ç”¨äººå·¥æŒ‘é€‰åçš„å¤šè¯­è¨€STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-multilingual-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-multilingual-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­è‹±æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œå¤šè¯­è¨€è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `shibing624/text2vec-bge-large-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`BAAI/bge-large-zh-noinstruct`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡æµ‹è¯•é›†è¯„ä¼°ç›¸å¯¹äºåŸæ¨¡å‹æ•ˆæœæœ‰æå‡ï¼Œåœ¨çŸ­æ–‡æœ¬åŒºåˆ†åº¦ä¸Šæå‡æ˜æ˜¾ï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `w2v-light-tencent-chinese`æ˜¯è…¾è®¯è¯å‘é‡çš„Word2Vecæ¨¡å‹ï¼ŒCPUåŠ è½½ä½¿ç”¨ï¼Œé€‚ç”¨äºä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œç¼ºå°‘æ•°æ®çš„å†·å¯åŠ¨æƒ…å†µ
- å„é¢„è®­ç»ƒæ¨¡å‹å‡å¯ä»¥é€šè¿‡transformersè°ƒç”¨ï¼Œå¦‚MacBERTæ¨¡å‹ï¼š`--model_name hfl/chinese-macbert-base` æˆ–è€…robertaæ¨¡å‹ï¼š`--model_name uer/roberta-medium-wwm-chinese-cluecorpussmall`
- ä¸ºæµ‹è¯„æ¨¡å‹çš„é²æ£’æ€§ï¼ŒåŠ å…¥äº†æœªè®­ç»ƒè¿‡çš„SOHUæµ‹è¯•é›†ï¼Œç”¨äºæµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼›ä¸ºè¾¾åˆ°å¼€ç®±å³ç”¨çš„å®ç”¨æ•ˆæœï¼Œä½¿ç”¨äº†æœé›†åˆ°çš„å„ä¸­æ–‡åŒ¹é…æ•°æ®é›†ï¼Œæ•°æ®é›†ä¹Ÿä¸Šä¼ åˆ°HF datasets[é“¾æ¥è§ä¸‹æ–¹](#æ•°æ®é›†)
- ä¸­æ–‡åŒ¹é…ä»»åŠ¡å®éªŒè¡¨æ˜ï¼Œpoolingæœ€ä¼˜æ˜¯`EncoderType.FIRST_LAST_AVG`å’Œ`EncoderType.MEAN`ï¼Œä¸¤è€…é¢„æµ‹æ•ˆæœå·®å¼‚å¾ˆå°
- ä¸­æ–‡åŒ¹é…è¯„æµ‹ç»“æœå¤ç°ï¼Œå¯ä»¥ä¸‹è½½ä¸­æ–‡åŒ¹é…æ•°æ®é›†åˆ°`examples/data`ï¼Œè¿è¡Œ [tests/model_spearman.py](https://github.com/shibing624/text2vec/blob/master/tests/model_spearman.py) ä»£ç å¤ç°è¯„æµ‹ç»“æœ
- QPSçš„GPUæµ‹è¯•ç¯å¢ƒæ˜¯Tesla V100ï¼Œæ˜¾å­˜32GB

æ¨¡å‹è®­ç»ƒå®éªŒæŠ¥å‘Šï¼š[å®éªŒæŠ¥å‘Š](https://github.com/shibing624/text2vec/blob/master/docs/model_report.md)
## Demo

Official Demo: https://www.mulanai.com/product/short_text_sim/

HuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec

![](https://github.com/shibing624/text2vec/blob/master/docs/hf.png)

run example: [examples/gradio_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/gradio_demo.py) to see the demo:
```shell
python examples/gradio_demo.py
```

## Install
```shell
pip install torch # conda install pytorch
pip install -U text2vec
```

or

```shell
pip install torch # conda install pytorch
pip install -r requirements.txt

git clone https://github.com/shibing624/text2vec.git
cd text2vec
pip install --no-deps .
```

## Usage

### æ–‡æœ¬å‘é‡è¡¨å¾

åŸºäº`pretrained model`è®¡ç®—æ–‡æœ¬å‘é‡ï¼š

```zsh
>>> from text2vec import SentenceModel
>>> m = SentenceModel()
>>> m.encode("å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡")
Embedding shape: (768,)
```

example: [examples/computing_embeddings_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_demo.py)

```python
import sys

sys.path.append('..')
from text2vec import SentenceModel
from text2vec import Word2Vec


def compute_emb(model):
    # Embed a list of sentences
    sentences = [
        'å¡',
        'é“¶è¡Œå¡',
        'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
        'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',
        'This framework generates embeddings for each input sentence',
        'Sentences are passed as a list of string.',
        'The quick brown fox jumps over the lazy dog.'
    ]
    sentence_embeddings = model.encode(sentences)
    print(type(sentence_embeddings), sentence_embeddings.shape)

    # The result is a list of sentence embeddings as numpy arrays
    for sentence, embedding in zip(sentences, sentence_embeddings):
        print("Sentence:", sentence)
        print("Embedding shape:", embedding.shape)
        print("Embedding head:", embedding[:10])
        print()


if __name__ == "__main__":
    # ä¸­æ–‡å¥å‘é‡æ¨¡å‹(CoSENT)ï¼Œä¸­æ–‡è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
    t2v_model = SentenceModel("shibing624/text2vec-base-chinese")
    compute_emb(t2v_model)

    # æ”¯æŒå¤šè¯­è¨€çš„å¥å‘é‡æ¨¡å‹ï¼ˆCoSENTï¼‰ï¼Œå¤šè¯­è¨€ï¼ˆåŒ…æ‹¬ä¸­è‹±æ–‡ï¼‰è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èï¼Œæ”¯æŒfine-tuneç»§ç»­è®­ç»ƒ
    sbert_model = SentenceModel("shibing624/text2vec-base-multilingual")
    compute_emb(sbert_model)

    # ä¸­æ–‡è¯å‘é‡æ¨¡å‹(word2vec)ï¼Œä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œå†·å¯åŠ¨é€‚ç”¨
    w2v_model = Word2Vec("w2v-light-tencent-chinese")
    compute_emb(w2v_model)

```

output:
```
<class 'numpy.ndarray'> (7, 768)
Sentence: å¡
Embedding shape: (768,)

Sentence: é“¶è¡Œå¡
Embedding shape: (768,)
 ... 
```

- è¿”å›å€¼`embeddings`æ˜¯`numpy.ndarray`ç±»å‹ï¼Œshapeä¸º`(sentences_size, model_embedding_size)`ï¼Œä¸‰ä¸ªæ¨¡å‹ä»»é€‰ä¸€ç§å³å¯ï¼Œæ¨èç”¨ç¬¬ä¸€ä¸ªã€‚
- `shibing624/text2vec-base-chinese`æ¨¡å‹æ˜¯CoSENTæ–¹æ³•åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå¾—åˆ°çš„ï¼Œæ¨¡å‹å·²ç»ä¸Šä¼ åˆ°huggingfaceçš„
æ¨¡å‹åº“[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)ï¼Œ
æ˜¯`text2vec.SentenceModel`æŒ‡å®šçš„é»˜è®¤æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸Šé¢ç¤ºä¾‹è°ƒç”¨ï¼Œæˆ–è€…å¦‚ä¸‹æ‰€ç¤ºç”¨[transformersåº“](https://github.com/huggingface/transformers)è°ƒç”¨ï¼Œ
æ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.cache/huggingface/transformers`
- `w2v-light-tencent-chinese`æ˜¯é€šè¿‡gensimåŠ è½½çš„Word2Vecæ¨¡å‹ï¼Œä½¿ç”¨è…¾è®¯è¯å‘é‡è®¡ç®—å„å­—è¯çš„è¯å‘é‡ï¼Œå¥å­å‘é‡é€šè¿‡å•è¯è¯
å‘é‡å–å¹³å‡å€¼å¾—åˆ°ï¼Œæ¨¡å‹è‡ªåŠ¨ä¸‹è½½åˆ°æœ¬æœºè·¯å¾„ï¼š`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`
- `text2vec`æ”¯æŒå¤šå¡æ¨ç†(è®¡ç®—æ–‡æœ¬å‘é‡): [examples/computing_embeddings_multi_gpu_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/computing_embeddings_multi_gpu_demo.py)

#### Usage (HuggingFace Transformers)
Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 

First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.

example: [examples/use_origin_transformers_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/use_origin_transformers_demo.py)

```python
import os
import torch
from transformers import AutoTokenizer, AutoModel

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


# Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)


# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained('shibing624/text2vec-base-chinese')
model = AutoModel.from_pretrained('shibing624/text2vec-base-chinese')
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']
# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)
# Perform pooling. In this case, max pooling.
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
print("Sentence embeddings:")
print(sentence_embeddings)
```

#### Usage (sentence-transformers)
[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.

Install sentence-transformers:
```shell
pip install -U sentence-transformers
```
Then load model and predict:
```python
from sentence_transformers import SentenceTransformer

m = SentenceTransformer("shibing624/text2vec-base-chinese")
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']

sentence_embeddings = m.encode(sentences)
print("Sentence embeddings:")
print(sentence_embeddings)
```

#### `Word2Vec`è¯å‘é‡

æä¾›`Word2Vec`è¯å‘é‡ï¼Œè½»é‡ç‰ˆè…¾è®¯è¯å‘é‡`light_Tencent_AILab_ChineseEmbedding.bin`ï¼ŒäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œ111Mï¼Œæ˜¯ç®€åŒ–åçš„é«˜é¢‘143613ä¸ªè¯ï¼Œæ¯ä¸ªè¯å‘é‡è¿˜æ˜¯200ç»´ï¼ˆè·ŸåŸç‰ˆä¸€æ ·ï¼‰ï¼Œè¿è¡Œç¨‹åºï¼Œè‡ªåŠ¨ä¸‹è½½åˆ° `~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`

æ¨¡å‹åœ°å€ï¼š[Modelscope](https://modelscope.cn/models/lili666/text2vec-word2vec-tencent-chinese/summary) | [ç™¾åº¦äº‘ç›˜-å¯†ç :tawe](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) | [è°·æ­Œäº‘ç›˜](https://drive.google.com/u/0/uc?id=1iQo9tBb2NgFOBxx0fA16AZpSgc-bG_Rp&export=download)

### å‘½ä»¤è¡Œæ¨¡å¼ï¼ˆCLIï¼‰

æ”¯æŒæ‰¹é‡è·å–æ–‡æœ¬å‘é‡

code: [cli.py](https://github.com/shibing624/text2vec/blob/master/text2vec/cli.py)

```
> text2vec -h                                    
usage: text2vec [-h] --input_file INPUT_FILE [--output_file OUTPUT_FILE] [--model_type MODEL_TYPE] [--model_name MODEL_NAME] [--encoder_type ENCODER_TYPE]
                [--batch_size BATCH_SIZE] [--max_seq_length MAX_SEQ_LENGTH] [--chunk_size CHUNK_SIZE] [--device DEVICE]
                [--show_progress_bar SHOW_PROGRESS_BAR] [--normalize_embeddings NORMALIZE_EMBEDDINGS]

text2vec cli

optional arguments:
  -h, --help            show this help message and exit
  --input_file INPUT_FILE
                        input file path, text file, required
  --output_file OUTPUT_FILE
                        output file path, output csv file, default text_embs.csv
  --model_type MODEL_TYPE
                        model type: sentencemodel, word2vec, default sentencemodel
  --model_name MODEL_NAME
                        model name or path, default shibing624/text2vec-base-chinese
  --encoder_type ENCODER_TYPE
                        encoder type: MEAN, CLS, POOLER, FIRST_LAST_AVG, LAST_AVG, default MEAN
  --batch_size BATCH_SIZE
                        batch size, default 32
  --max_seq_length MAX_SEQ_LENGTH
                        max sequence length, default 256
  --chunk_size CHUNK_SIZE
                        chunk size to save partial results, default 1000
  --device DEVICE       device: cpu, cuda, default None
  --show_progress_bar SHOW_PROGRESS_BAR
                        show progress bar, default True
  --normalize_embeddings NORMALIZE_EMBEDDINGS
                        normalize embeddings, default False
  --multi_gpu MULTI_GPU
                        multi gpu, default False
```

runï¼š

```shell
pip install text2vec -U
text2vec --input_file input.txt --output_file out.csv --batch_size 128 --multi_gpu True
```

> è¾“å…¥æ–‡ä»¶ï¼ˆrequiredï¼‰ï¼š`input.txt`ï¼Œformatï¼šä¸€å¥è¯ä¸€è¡Œçš„å¥å­æ–‡æœ¬ã€‚

## ä¸‹æ¸¸ä»»åŠ¡
### 1. å¥å­ç›¸ä¼¼åº¦è®¡ç®—

example: [examples/semantic_text_similarity_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_text_similarity_demo.py)

```python
import sys

sys.path.append('..')
from text2vec import Similarity

# Two lists of sentences
sentences1 = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
              'The cat sits outside',
              'A man is playing guitar',
              'The new movie is awesome']

sentences2 = ['èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',
              'The dog plays in the garden',
              'A woman watches TV',
              'The new movie is so great']

sim_model = Similarity()
for i in range(len(sentences1)):
    for j in range(len(sentences2)):
        score = sim_model.get_score(sentences1[i], sentences2[j])
        print("{} \t\t {} \t\t Score: {:.4f}".format(sentences1[i], sentences2[j], score))
```

output:
```shell
å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ 		 èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ 		 Score: 0.9477
å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ 		 The dog plays in the garden 		 Score: -0.1748
å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ 		 A woman watches TV 		 Score: -0.0839
å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡ 		 The new movie is so great 		 Score: -0.0044
The cat sits outside 		 èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ 		 Score: -0.0097
The cat sits outside 		 The dog plays in the garden 		 Score: 0.1908
The cat sits outside 		 A woman watches TV 		 Score: -0.0203
The cat sits outside 		 The new movie is so great 		 Score: 0.0302
A man is playing guitar 		 èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ 		 Score: -0.0010
A man is playing guitar 		 The dog plays in the garden 		 Score: 0.1062
A man is playing guitar 		 A woman watches TV 		 Score: 0.0055
A man is playing guitar 		 The new movie is so great 		 Score: 0.0097
The new movie is awesome 		 èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ 		 Score: 0.0302
The new movie is awesome 		 The dog plays in the garden 		 Score: -0.0160
The new movie is awesome 		 A woman watches TV 		 Score: 0.1321
The new movie is awesome 		 The new movie is so great 		 Score: 0.9591
```

> å¥å­ä½™å¼¦ç›¸ä¼¼åº¦å€¼`score`èŒƒå›´æ˜¯[-1, 1]ï¼Œå€¼è¶Šå¤§è¶Šç›¸ä¼¼ã€‚

### 2. æ–‡æœ¬åŒ¹é…æœç´¢

ä¸€èˆ¬åœ¨æ–‡æ¡£å€™é€‰é›†ä¸­æ‰¾ä¸queryæœ€ç›¸ä¼¼çš„æ–‡æœ¬ï¼Œå¸¸ç”¨äºQAåœºæ™¯çš„é—®å¥ç›¸ä¼¼åŒ¹é…ã€æ–‡æœ¬ç›¸ä¼¼æ£€ç´¢ç­‰ä»»åŠ¡ã€‚


example: [examples/semantic_search_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/semantic_search_demo.py)

```python
import sys

sys.path.append('..')
from text2vec import SentenceModel, cos_sim, semantic_search

embedder = SentenceModel()

# Corpus with example sentences
corpus = [
    'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡',
    'æˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘—',
    'A man is eating food.',
    'A man is eating a piece of bread.',
    'The girl is carrying a baby.',
    'A man is riding a horse.',
    'A woman is playing violin.',
    'Two men pushed carts through the woods.',
    'A man is riding a white horse on an enclosed ground.',
    'A monkey is playing drums.',
    'A cheetah is running behind its prey.'
]
corpus_embeddings = embedder.encode(corpus)

# Query sentences:
queries = [
    'å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
    'A man is eating pasta.',
    'Someone in a gorilla costume is playing a set of drums.',
    'A cheetah chases prey on across a field.']

for query in queries:
    query_embedding = embedder.encode(query)
    hits = semantic_search(query_embedding, corpus_embeddings, top_k=5)
    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")
    hits = hits[0]  # Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
```
output:
```shell
Query: å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡
Top 5 most similar sentences in corpus:
èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡ (Score: 0.9477)
æˆ‘ä»€ä¹ˆæ—¶å€™å¼€é€šäº†èŠ±å‘— (Score: 0.3635)
A man is eating food. (Score: 0.0321)
A man is riding a horse. (Score: 0.0228)
Two men pushed carts through the woods. (Score: 0.0090)

======================
Query: A man is eating pasta.
Top 5 most similar sentences in corpus:
A man is eating food. (Score: 0.6734)
A man is eating a piece of bread. (Score: 0.4269)
A man is riding a horse. (Score: 0.2086)
A man is riding a white horse on an enclosed ground. (Score: 0.1020)
A cheetah is running behind its prey. (Score: 0.0566)

======================
Query: Someone in a gorilla costume is playing a set of drums.
Top 5 most similar sentences in corpus:
A monkey is playing drums. (Score: 0.8167)
A cheetah is running behind its prey. (Score: 0.2720)
A woman is playing violin. (Score: 0.1721)
A man is riding a horse. (Score: 0.1291)
A man is riding a white horse on an enclosed ground. (Score: 0.1213)

======================
Query: A cheetah chases prey on across a field.
Top 5 most similar sentences in corpus:
A cheetah is running behind its prey. (Score: 0.9147)
A monkey is playing drums. (Score: 0.2655)
A man is riding a horse. (Score: 0.1933)
A man is riding a white horse on an enclosed ground. (Score: 0.1733)
A man is eating food. (Score: 0.0329)
```

 

## ä¸‹æ¸¸ä»»åŠ¡æ”¯æŒåº“
**similaritiesåº“[æ¨è]**

æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å’Œæ–‡æœ¬åŒ¹é…æœç´¢ä»»åŠ¡ï¼Œæ¨èä½¿ç”¨ [similaritiesåº“](https://github.com/shibing624/similarities) ï¼Œå…¼å®¹æœ¬é¡¹ç›®releaseçš„
Word2vecã€SBERTã€Cosentç±»è¯­ä¹‰åŒ¹é…æ¨¡å‹ï¼Œè¿˜æ”¯æŒäº¿çº§å›¾æ–‡æœç´¢ï¼Œæ”¯æŒ**æ–‡æœ¬è¯­ä¹‰å»é‡**ï¼Œ**å›¾ç‰‡å»é‡**ç­‰åŠŸèƒ½ã€‚

å®‰è£…ï¼š
```pip install -U similarities```

å¥å­ç›¸ä¼¼åº¦è®¡ç®—ï¼š
```python
from similarities import BertSimilarity

m = BertSimilarity()
r = m.similarity('å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')
print(f"similarity score: {float(r)}")  # similarity score: 0.855146050453186
```

## Models

### CoSENT model

CoSENTï¼ˆCosine Sentenceï¼‰æ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œåœ¨Sentence-BERTä¸Šæ”¹è¿›äº†CosineRankLossçš„å¥å‘é‡æ–¹æ¡ˆ


Network structure:

Training:

<img src="docs/cosent_train.png" width="300" />


Inference:

<img src="docs/inference.png" width="300" />

#### CoSENT ç›‘ç£æ¨¡å‹
è®­ç»ƒå’Œé¢„æµ‹CoSENTæ¨¡å‹ï¼š

- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹

example: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)

```shell
cd examples
python training_sup_text_matching_model.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-cosent
```

- åœ¨èš‚èšé‡‘èåŒ¹é…æ•°æ®é›†ATECä¸Šè®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹

æ”¯æŒè¿™äº›ä¸­æ–‡åŒ¹é…æ•°æ®é›†çš„ä½¿ç”¨ï¼š'ATEC', 'STS-B', 'BQ', 'LCQMC', 'PAWSX'ï¼Œå…·ä½“å‚è€ƒHuggingFace datasets [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)
```shell
python training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent
```

- åœ¨è‡ªæœ‰ä¸­æ–‡æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹

example: [examples/training_sup_text_matching_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_mydata.py)

å•å¡è®­ç»ƒï¼š
```shell
CUDA_VISIBLE_DEVICES=0 python training_sup_text_matching_model_mydata.py --do_train --do_predict
```

å¤šå¡è®­ç»ƒï¼š
```shell
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node 2  training_sup_text_matching_model_mydata.py --do_train --do_predict --output_dir outputs/STS-B-text2vec-macbert-v1 --batch_size 64 --bf16 --data_parallel 
```

è®­ç»ƒé›†æ ¼å¼å‚è€ƒ[examples/data/STS-B/STS-B.valid.data](https://github.com/shibing624/text2vec/blob/master/examples/data/STS-B/STS-B.valid.data)

```shell
sentence1   sentence2   label
ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚	ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚	2
ä¸€ç¾¤ç”·äººåœ¨æµ·æ»©ä¸Šè¸¢è¶³çƒã€‚	ä¸€ç¾¤ç”·å­©åœ¨æµ·æ»©ä¸Šè¸¢è¶³çƒã€‚	3
ä¸€ä¸ªå¥³äººåœ¨æµ‹é‡å¦ä¸€ä¸ªå¥³äººçš„è„šè¸ã€‚	å¥³äººæµ‹é‡å¦ä¸€ä¸ªå¥³äººçš„è„šè¸ã€‚	5
```

`label`å¯ä»¥æ˜¯0ï¼Œ1æ ‡ç­¾ï¼Œ0ä»£è¡¨ä¸¤ä¸ªå¥å­ä¸ç›¸ä¼¼ï¼Œ1ä»£è¡¨ç›¸ä¼¼ï¼›ä¹Ÿå¯ä»¥æ˜¯0-5çš„è¯„åˆ†ï¼Œè¯„åˆ†è¶Šé«˜ï¼Œè¡¨ç¤ºä¸¤ä¸ªå¥å­è¶Šç›¸ä¼¼ã€‚æ¨¡å‹éƒ½èƒ½æ”¯æŒã€‚


- åœ¨è‹±æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`CoSENT`æ¨¡å‹

example: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)

```shell
cd examples
python training_sup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased  --output_dir ./outputs/STS-B-en-cosent
```

#### CoSENT æ— ç›‘ç£æ¨¡å‹
- åœ¨è‹±æ–‡NLIæ•°æ®é›†è®­ç»ƒ`CoSENT`æ¨¡å‹ï¼Œåœ¨STS-Bæµ‹è¯•é›†è¯„ä¼°æ•ˆæœ

example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)

```shell
cd examples
python training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent
```


### Sentence-BERT model

Sentence-BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼Œè¡¨å¾å¼å¥å‘é‡è¡¨ç¤ºæ–¹æ¡ˆ

Network structure:

Training:

<img src="docs/sbert_train.png" width="300" />


Inference:

<img src="docs/sbert_inference.png" width="300" />

#### SentenceBERT ç›‘ç£æ¨¡å‹
- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`SBERT`æ¨¡å‹

example: [examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)

```shell
cd examples
python training_sup_text_matching_model.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-sbert
```
- åœ¨è‹±æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`SBERT`æ¨¡å‹

example: [examples/training_sup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_en.py)

```shell
cd examples
python training_sup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-sbert
```

#### SentenceBERT æ— ç›‘ç£æ¨¡å‹
- åœ¨è‹±æ–‡NLIæ•°æ®é›†è®­ç»ƒ`SBERT`æ¨¡å‹ï¼Œåœ¨STS-Bæµ‹è¯•é›†è¯„ä¼°æ•ˆæœ

example: [examples/training_unsup_text_matching_model_en.py](https://github.com/shibing624/text2vec/blob/master/examples/training_unsup_text_matching_model_en.py)

```shell
cd examples
python training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert
```

### BERT-Match model
BERTæ–‡æœ¬åŒ¹é…æ¨¡å‹ï¼ŒåŸç”ŸBERTåŒ¹é…ç½‘ç»œç»“æ„ï¼Œäº¤äº’å¼å¥å‘é‡åŒ¹é…æ¨¡å‹

Network structure:

Training and inference:

<img src="docs/bert-fc-train.png" width="300" />

è®­ç»ƒè„šæœ¬åŒä¸Š[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ã€‚



### BGE model

#### BGE ç›‘ç£æ¨¡å‹
- åœ¨ä¸­æ–‡STS-Bæ•°æ®é›†è®­ç»ƒå’Œè¯„ä¼°`BGE`æ¨¡å‹

example: [examples/training_bge_model_mydata.py](https://github.com/shibing624/text2vec/blob/master/examples/training_bge_model_mydata.py)

```shell
cd examples
python training_bge_model_mydata.py --model_arch bge --do_train --do_predict --num_epochs 4 --output_dir ./outputs/STS-B-bge-v1 --batch_size 4 --save_model_every_epoch --bf16
```

- è‡ªå»ºBGEè®­ç»ƒé›†

BGEæ¨¡å‹å¾®è°ƒè®­ç»ƒï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œè¾“å…¥æ•°æ®çš„æ ¼å¼æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„' (query, positive, negative) '

```shell
cd examples/data
python build_zh_bge_dataset.py
python hard_negatives_mine.py
```
1. `build_zh_bge_dataset.py` åŸºäºä¸­æ–‡STS-Bç”Ÿæˆä¸‰å…ƒç»„è®­ç»ƒé›†ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json lines
{"query":"ä¸€ä¸ªç”·äººæ­£åœ¨å¾€é”…é‡Œå€’æ²¹ã€‚","pos":["ä¸€ä¸ªç”·äººæ­£åœ¨å¾€é”…é‡Œå€’æ²¹ã€‚"],"neg":["äº²ä¿„å†›é˜Ÿè¿›å…¥å…‹é‡Œç±³äºšä¹Œå…‹å…°æµ·å†›åŸºåœ°","é…æœ‰æœ¨åˆ¶å®¶å…·çš„ä¼˜é›…é¤å…ã€‚","é©¬é›…ç“¦è’‚è¦æ±‚æ€»ç»Ÿç»Ÿæ²»æŸ¥è°Ÿå’Œå…‹ä»€ç±³å°”","éå…¸è¿˜å¤ºå»äº†å¤šä¼¦å¤šåœ°åŒº44äººçš„ç”Ÿå‘½ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸¤åæŠ¤å£«å’Œä¸€ååŒ»ç”Ÿã€‚","åœ¨ä¸€æ¬¡é‡‡è®¿ä¸­ï¼Œèº«ä¸ºçŠ¯ç½ªå­¦å®¶çš„å¸Œåˆ©è¯´ï¼Œè¿™é‡Œå’Œå…¨å›½å„åœ°çš„è®¸å¤šè®®å‘˜éƒ½å¯¹æ­»åˆ‘æŠ±æœ‰æˆ’å¿ƒã€‚","è±šé¼ åƒèƒ¡èåœã€‚","ç‹—å˜´é‡Œå¼ç€ä¸€æ ¹æ£å­åœ¨æ°´ä¸­æ¸¸æ³³ã€‚","æ‹‰é‡ŒÂ·ä½©å¥‡è¯´Androidå¾ˆé‡è¦ï¼Œä¸æ˜¯å…³é”®","æ³•å›½ã€æ¯”åˆ©æ—¶ã€å¾·å›½ã€ç‘å…¸ã€æ„å¤§åˆ©å’Œè‹±å›½ä¸ºå°åº¦è®¡åˆ’å‘ç¼…ç”¸å‡ºå”®çš„å…ˆè¿›è½»å‹ç›´å‡æœºæä¾›é›¶éƒ¨ä»¶å’ŒæŠ€æœ¯ã€‚","å·´æ—èµ›é©¬ä¼šåœ¨åŠ¨ä¹±ä¸­è¿›è¡Œ"]}
```
2. `hard_negatives_mine.py` ä½¿ç”¨faissç›¸ä¼¼åŒ¹é…ï¼ŒæŒ–æ˜éš¾è´Ÿä¾‹ã€‚


### æ¨¡å‹è’¸é¦ï¼ˆModel Distillationï¼‰

ç”±äºtext2vecè®­ç»ƒçš„æ¨¡å‹å¯ä»¥ä½¿ç”¨[sentence-transformers](https://github.com/UKPLab/sentence-transformers)åº“åŠ è½½ï¼Œæ­¤å¤„å¤ç”¨å…¶æ¨¡å‹è’¸é¦æ–¹æ³•[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)ã€‚

1. æ¨¡å‹é™ç»´ï¼Œå‚è€ƒ[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)ä½¿ç”¨PCAå¯¹æ¨¡å‹è¾“å‡ºembeddingé™ç»´ï¼Œå¯å‡å°‘milvusç­‰å‘é‡æ£€ç´¢æ•°æ®åº“çš„å­˜å‚¨å‹åŠ›ï¼Œè¿˜èƒ½è½»å¾®æå‡æ¨¡å‹æ•ˆæœã€‚
2. æ¨¡å‹è’¸é¦ï¼Œå‚è€ƒ[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)ä½¿ç”¨è’¸é¦æ–¹æ³•ï¼Œå°†Teacherå¤§æ¨¡å‹è’¸é¦åˆ°æ›´å°‘layerså±‚æ•°çš„studentæ¨¡å‹ä¸­ï¼Œåœ¨æƒè¡¡æ•ˆæœçš„æƒ…å†µä¸‹ï¼Œå¯å¤§å¹…æå‡æ¨¡å‹é¢„æµ‹é€Ÿåº¦ã€‚

### æ¨¡å‹éƒ¨ç½²

æä¾›ä¸¤ç§éƒ¨ç½²æ¨¡å‹ï¼Œæ­å»ºæœåŠ¡çš„æ–¹æ³•ï¼š 1ï¼‰åŸºäºJinaæ­å»ºgRPCæœåŠ¡ã€æ¨èã€‘ï¼›2ï¼‰åŸºäºFastAPIæ­å»ºåŸç”ŸHttpæœåŠ¡ã€‚

#### JinaæœåŠ¡
é‡‡ç”¨C/Sæ¨¡å¼æ­å»ºé«˜æ€§èƒ½æœåŠ¡ï¼Œæ”¯æŒdockeräº‘åŸç”Ÿï¼ŒgRPC/HTTP/WebSocketï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹åŒæ—¶é¢„æµ‹ï¼ŒGPUå¤šå¡å¤„ç†ã€‚

- å®‰è£…ï¼š
```pip install jina```

- å¯åŠ¨æœåŠ¡ï¼š

example: [examples/jina_server_demo.py](examples/jina_server_demo.py)
```python
from jina import Flow

port = 50001
f = Flow(port=port).add(
    uses='jinahub://Text2vecEncoder',
    uses_with={'model_name': 'shibing624/text2vec-base-chinese'}
)

with f:
    # backend server forever
    f.block()
```

è¯¥æ¨¡å‹é¢„æµ‹æ–¹æ³•ï¼ˆexecutorï¼‰å·²ç»ä¸Šä¼ åˆ°[JinaHub](https://hub.jina.ai/executor/eq45c9uq)ï¼Œé‡Œé¢åŒ…æ‹¬dockerã€k8séƒ¨ç½²æ–¹æ³•ã€‚

- è°ƒç”¨æœåŠ¡ï¼š


```python
from jina import Client
from docarray import Document, DocumentArray

port = 50001

c = Client(port=port)

data = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡',
        'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']
print("data:", data)
print('data embs:')
r = c.post('/', inputs=DocumentArray([Document(text='å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡'), Document(text='èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡')]))
print(r.embeddings)
```

æ‰¹é‡è°ƒç”¨æ–¹æ³•è§example: [examples/jina_client_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/jina_client_demo.py)


#### FastAPIæœåŠ¡

- å®‰è£…ï¼š
```pip install fastapi uvicorn```

- å¯åŠ¨æœåŠ¡ï¼š

example: [examples/fastapi_server_demo.py](https://github.com/shibing624/text2vec/blob/master/examples/fastapi_server_demo.py)
```shell
cd examples
python fastapi_server_demo.py
```

- è°ƒç”¨æœåŠ¡ï¼š
```shell
curl -X 'GET' \
  'http://0.0.0.0:8001/emb?q=hello' \
  -H 'accept: application/json'
```


## Dataset

- æœ¬é¡¹ç›®releaseçš„æ•°æ®é›†ï¼š

| Dataset                    | Introduce                                                                | Download Link                                                                                                                                                                                                                                                                                         |
|:---------------------------|:-------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| shibing624/nli-zh-all      | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®åˆé›†ï¼Œæ•´åˆäº†æ–‡æœ¬æ¨ç†ï¼Œç›¸ä¼¼ï¼Œæ‘˜è¦ï¼Œé—®ç­”ï¼ŒæŒ‡ä»¤å¾®è°ƒç­‰ä»»åŠ¡çš„820ä¸‡é«˜è´¨é‡æ•°æ®ï¼Œå¹¶è½¬åŒ–ä¸ºåŒ¹é…æ ¼å¼æ•°æ®é›†                | [https://huggingface.co/datasets/shibing624/nli-zh-all](https://huggingface.co/datasets/shibing624/nli-zh-all)                                                                                                                                                                                        |
| shibing624/snli-zh         | ä¸­æ–‡SNLIå’ŒMultiNLIæ•°æ®é›†ï¼Œç¿»è¯‘è‡ªè‹±æ–‡SNLIå’ŒMultiNLI                                    | [https://huggingface.co/datasets/shibing624/snli-zh](https://huggingface.co/datasets/shibing624/snli-zh)                                                                                                                                                                                              |
| shibing624/nli_zh          | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®é›†ï¼Œæ•´åˆäº†ä¸­æ–‡ATECã€BQã€LCQMCã€PAWSXã€STS-Bå…±5ä¸ªä»»åŠ¡çš„æ•°æ®é›†                        | [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh) </br> or </br> [ç™¾åº¦ç½‘ç›˜(æå–ç :qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ) </br> or </br> [github](https://github.com/shibing624/text2vec/releases/download/1.1.2/senteval_cn.zip) </br> |
| shibing624/sts-sohu2021    | ä¸­æ–‡è¯­ä¹‰åŒ¹é…æ•°æ®é›†ï¼Œ2021æœç‹æ ¡å›­æ–‡æœ¬åŒ¹é…ç®—æ³•å¤§èµ›æ•°æ®é›†                                            | [https://huggingface.co/datasets/shibing624/sts-sohu2021](https://huggingface.co/datasets/shibing624/sts-sohu2021)                                                                                                                                                                                    |
| ATEC                       | ä¸­æ–‡ATECæ•°æ®é›†ï¼Œèš‚èšé‡‘æœQ-Qpairæ•°æ®é›†                                                 | [ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC)                                                                                                                                                                                                                                 |
| BQ                         | ä¸­æ–‡BQ(Bank Question)æ•°æ®é›†ï¼Œé“¶è¡ŒQ-Qpairæ•°æ®é›†                                      | [BQ](http://icrc.hitsz.edu.cn/info/1037/1162.htm)                                                                                                                                                                                                                                                     |
| LCQMC                      | ä¸­æ–‡LCQMC(large-scale Chinese question matching corpus)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†      | [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)                                                                                                                                                                                                                                               |
| PAWSX                      | ä¸­æ–‡PAWS(Paraphrase Adversaries from Word Scrambling)æ•°æ®é›†ï¼ŒQ-Qpairæ•°æ®é›†        | [PAWSX](https://arxiv.org/abs/1908.11828)                                                                                                                                                                                                                                                             |
| STS-B                      | ä¸­æ–‡STS-Bæ•°æ®é›†ï¼Œä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ï¼Œä»è‹±æ–‡STS-Bç¿»è¯‘ä¸ºä¸­æ–‡çš„æ•°æ®é›†                                 | [STS-B](https://github.com/pluto-junzeng/CNSD)                                                                                                                                                                                                                                                        |


å¸¸ç”¨è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼š

- è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šmulti_nli: https://huggingface.co/datasets/multi_nli
- è‹±æ–‡åŒ¹é…æ•°æ®é›†ï¼šsnli: https://huggingface.co/datasets/snli
- https://huggingface.co/datasets/metaeval/cnli
- https://huggingface.co/datasets/mteb/stsbenchmark-sts
- https://huggingface.co/datasets/JeremiahZ/simcse_sup_nli
- https://huggingface.co/datasets/MoritzLaurer/multilingual-NLI-26lang-2mil7


æ•°æ®é›†ä½¿ç”¨ç¤ºä¾‹ï¼š
```shell
pip install datasets
```

```python
from datasets import load_dataset

dataset = load_dataset("shibing624/nli_zh", "STS-B") # ATEC or BQ or LCQMC or PAWSX or STS-B
print(dataset)
print(dataset['test'][0])
```

output:
```shell
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label'],
        num_rows: 5231
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label'],
        num_rows: 1458
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label'],
        num_rows: 1361
    })
})
{'sentence1': 'ä¸€ä¸ªå¥³å­©åœ¨ç»™å¥¹çš„å¤´å‘åšå‘å‹ã€‚', 'sentence2': 'ä¸€ä¸ªå¥³å­©åœ¨æ¢³å¤´ã€‚', 'label': 2}
```

## Contact

- Issue(å»ºè®®)ï¼š[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)
- é‚®ä»¶æˆ‘ï¼šxuming: xuming624@qq.com
- å¾®ä¿¡æˆ‘ï¼šåŠ æˆ‘*å¾®ä¿¡å·ï¼šxuming624, å¤‡æ³¨ï¼šå§“å-å…¬å¸-NLP* è¿›NLPäº¤æµç¾¤ã€‚

<img src="docs/wechat.jpeg" width="200" />


## Citation

å¦‚æœä½ åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†text2vecï¼Œè¯·æŒ‰å¦‚ä¸‹æ ¼å¼å¼•ç”¨ï¼š

APA:
```latex
Xu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec
```

BibTeX:
```latex
@misc{Text2vec,
  author = {Ming Xu},
  title = {Text2vec: Text to vector toolkit},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/shibing624/text2vec}},
}
```

## License


æˆæƒåè®®ä¸º [The Apache License 2.0](LICENSE)ï¼Œå¯å…è´¹ç”¨åšå•†ä¸šç”¨é€”ã€‚è¯·åœ¨äº§å“è¯´æ˜ä¸­é™„åŠ text2vecçš„é“¾æ¥å’Œæˆæƒåè®®ã€‚


## Contribute
é¡¹ç›®ä»£ç è¿˜å¾ˆç²—ç³™ï¼Œå¦‚æœå¤§å®¶å¯¹ä»£ç æœ‰æ‰€æ”¹è¿›ï¼Œæ¬¢è¿æäº¤å›æœ¬é¡¹ç›®ï¼Œåœ¨æäº¤ä¹‹å‰ï¼Œæ³¨æ„ä»¥ä¸‹ä¸¤ç‚¹ï¼š

 - åœ¨`tests`æ·»åŠ ç›¸åº”çš„å•å…ƒæµ‹è¯•
 - ä½¿ç”¨`python -m pytest -v`æ¥è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•ï¼Œç¡®ä¿æ‰€æœ‰å•æµ‹éƒ½æ˜¯é€šè¿‡çš„

ä¹‹åå³å¯æäº¤PRã€‚

## References
- [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸Šï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10335164.html)
- [å°†å¥å­è¡¨ç¤ºä¸ºå‘é‡ï¼ˆä¸‹ï¼‰ï¼šæ— ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ï¼ˆsentence embeddingï¼‰](https://www.cnblogs.com/llhthinker/p/10341841.html)
- [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)
- [å››ç§è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ–¹æ³•å¯¹æ¯”[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)
- [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)
- [CoSENTï¼šæ¯”Sentence-BERTæ›´æœ‰æ•ˆçš„å¥å‘é‡æ–¹æ¡ˆ](https://kexue.fm/archives/8847)
- [è°ˆè°ˆæ–‡æœ¬åŒ¹é…å’Œå¤šè½®æ£€ç´¢](https://zhuanlan.zhihu.com/p/111769969)
- [Sentence-transformers](https://www.sbert.net/examples/applications/computing-embeddings/README.html)
- [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741)
